<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Blog of Bernardas Ališauskas</title><link>https://granitosaurus.github.io/</link><description>Python programmer and a goof who loves free software</description><lastBuildDate>Mon, 10 Oct 2016 00:00:00 +0200</lastBuildDate><item><title>How to parse complicated json trees.</title><link>https://granitosaurus.github.io/crawling-json.html</link><description>&lt;p&gt;Often when web-crawling you can find access to website's api which provides direct JSON of a product, however it's not always so easy to find what you need in what could be a multi-layer mess of a json.&lt;/p&gt;
&lt;p&gt;In this blog-post I'll cover few tools and ways to deal with really ugly json trees that you probably don't want to iterate through manually using dictionary key indices.&lt;br /&gt;
&lt;strong&gt;If you don't care about the research you can just skip to the &lt;a href="#right"&gt;right tool&lt;/a&gt; and &lt;a href="#solving"&gt;solving of the real life case&lt;/a&gt; sections at the end&lt;/strong&gt;.&lt;/p&gt;
&lt;h1&gt;Cause&lt;/h1&gt;
&lt;p&gt;Often websites, especially the ones that sell various products tend to overcomplicate their apis by stacking everything in one huge json tree that is at least 10 layers deep and is impossible to understand for an outsider or maybe even other developers in the company.&lt;/p&gt;
&lt;p&gt;In this case we'll take a look at small examples of &lt;a href="http://ah.nl"&gt;http://ah.nl&lt;/a&gt; responses and how can we deal with them without spending hours trying to reverse engineer the whole process.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example info&lt;/strong&gt;:&lt;br /&gt;
Product url: &lt;a href="http://www.ah.nl/producten/product/wi166580/maggi-opkikker-rundvlees"&gt;http://www.ah.nl/producten/product/wi166580/maggi-opkikker-rundvlees&lt;/a&gt;&lt;br /&gt;
Product api response: &lt;a href="https://ptpb.pw/aZ_S"&gt;https://ptpb.pw/aZ_S&lt;/a&gt;&lt;br /&gt;
If you put this response through some json visual tool like &lt;a href="http://jsonviewer.stack.hu/"&gt;http://jsonviewer.stack.hu/&lt;/a&gt; you'll notice what a huge mess it is: &lt;/p&gt;
&lt;p&gt;&lt;img alt="example json view" src="https://granitosaurus.github.io/images/json-crawling.png" /&gt;&lt;/p&gt;
&lt;p&gt;Multiple layers, multiple elements, list in a dict in a list in a dict and to parse that you'd end up doing something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;items&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;_embedded&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lanes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;_embedded&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;items&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And that's just half-way through the tree. For example to find the sku you'd have to use something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sku = data[&amp;#39;_embedded&amp;#39;][&amp;#39;lanes&amp;#39;][4][&amp;#39;_embedded&amp;#39;][&amp;#39;items&amp;#39;][0][&amp;#39;_embedded&amp;#39;][&amp;#39;product&amp;#39;][&amp;#39;id&amp;#39;]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that's with hard-coding of list indices which are very likely to change for every product, so on top of that ugly line above you'd have to use multiple list comprehensions to find the correct list item from the &lt;code&gt;lanes&lt;/code&gt; or &lt;code&gt;items&lt;/code&gt; lists.  This is bad, ugly, unreliable and extremely painful to work with.&lt;/p&gt;
&lt;h1&gt;Tools to Solve This&lt;/h1&gt;
&lt;p&gt;There are several ways this can be approaches and let me spoil it for you, majority of them are bad, so we'll start off with those.&lt;/p&gt;
&lt;p&gt;To demonstrate these tools better we'll be parsing this simple json:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = &amp;quot;&amp;quot;&amp;quot;{
    &amp;quot;one&amp;quot;: {
        &amp;quot;two&amp;quot;: [{
            &amp;quot;four&amp;quot;: {
                &amp;quot;name&amp;quot;: &amp;quot;four1_name&amp;quot;
            }
        }, {
            &amp;quot;four&amp;quot;: {
                &amp;quot;name&amp;quot;: &amp;quot;four2_name&amp;quot;
            }
        }]
    }
}&amp;quot;&amp;quot;&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;All examples below are also available on iPython notebook if you want to mess around with them yourself &lt;a href="https://granitosaurus.github.io/data/crawling-json_examples.ipynb"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Wrong: Flattening The Json&lt;/h2&gt;
&lt;p&gt;At first glance this might appear as an obvious solution - just flatten everything to the first level! However this brings out a huge issue with keys. Because every key has to be unique, when flattening the dictionary you need to merge several keys into one to preserve the tree order.
If we were to flatten our &lt;code&gt;data&lt;/code&gt;, it would end up looking like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = &amp;quot;&amp;quot;&amp;quot;{
    &amp;quot;one_two_four1_name&amp;quot;: &amp;quot;four1_name&amp;quot;,
    &amp;quot;one_two_four2_name&amp;quot;: &amp;quot;four2_name&amp;quot;,
    }&amp;quot;&amp;quot;&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In a way you might think it looks nice, but the truth is that it's really unpredictable and hard to parse in a more complex context, since you can only select individual values. This might be useful for some edge cases where you only need 1 field the json tree is only two or tree levels deep, but otherwise it's not worth bothering with.&lt;/p&gt;
&lt;h2&gt;Wrong: Jmespath, JSONPath and JSONiq etc.&lt;/h2&gt;
&lt;p&gt;These few libraries in a way designed specifically to solve this issue. It seems that json is notoriously bad when it comes to this issue, so tools like theses are dime a dozen on github and while they are great, they fall short when in comes to web-crawling or similar use cases.   &lt;/p&gt;
&lt;p&gt;However there are two major issues with these tools:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First one being that some of them like the very &lt;code&gt;Jmespath&lt;/code&gt;'s &lt;strong&gt; expressions root-bound&lt;/strong&gt; which means non-rooted expressions like xpath's &lt;code&gt;//product/name&lt;/code&gt; are not possible. This means that you need to write this ugly chain which is barely different to our dict key indices one:&lt;/p&gt;
&lt;p&gt;root.foo.bar[].foo2.bar2.product.mynode&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The only improvement here is that we can do a bit of recursion by calling &lt;code&gt;[]&lt;/code&gt; for every list element, saving us a few list comprehension calls. And it definitely looks nicer, doesn't it?&lt;br /&gt;
It is still bad though since at any point the tree might change and our crawler will break because we are root bound.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The second issue being is that all of them &lt;strong&gt;are extremely bloated&lt;/strong&gt;, to the point where they not only design their own parsing logic but also design their own syntax.   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When you are crawling a website you already have your own parsing tools to parse the html/xml (like &lt;code&gt;lxml&lt;/code&gt; or &lt;code&gt;parsel&lt;/code&gt;) and anything other would just introduce obvious redundancy and unnecessary complexity. &lt;/p&gt;
&lt;h2&gt;Almost Right: js2xml&lt;/h2&gt;
&lt;p&gt;First I'd like to start off with and give a shout out to a great tool called &lt;code&gt;js2xml&lt;/code&gt; which maintained by Scrapinghub. It pretty much does what it says - converts javascript code to an xml tree and it's &lt;em&gt;almost&lt;/em&gt; the right tool for our issue, almost.&lt;br /&gt;
Since json is part of javascript, this means we can use this tool to parse it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;lxml&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;etree&lt;/span&gt;
&lt;span class="c1"&gt;# we need to wrap our data json in variable declaration&lt;/span&gt;
&lt;span class="c1"&gt;# for js2xml to interpret it&lt;/span&gt;
&lt;span class="n"&gt;parsed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;js2xml&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;var foo = &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;etree&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tostring&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;parsed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pretty_print&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is the result:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;program&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;var&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;foo&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;object&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;one&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;object&amp;gt;&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;two&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;array&amp;gt;&lt;/span&gt;
              &lt;span class="nt"&gt;&amp;lt;object&amp;gt;&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;four&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
                  &lt;span class="nt"&gt;&amp;lt;object&amp;gt;&lt;/span&gt;
                    &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
                      &lt;span class="nt"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;four1_name&lt;span class="nt"&gt;&amp;lt;/string&amp;gt;&lt;/span&gt;
                    &lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
                  &lt;span class="nt"&gt;&amp;lt;/object&amp;gt;&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
              &lt;span class="nt"&gt;&amp;lt;/object&amp;gt;&lt;/span&gt;
              &lt;span class="nt"&gt;&amp;lt;object&amp;gt;&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;four&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
                  &lt;span class="nt"&gt;&amp;lt;object&amp;gt;&lt;/span&gt;
                    &lt;span class="nt"&gt;&amp;lt;property&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
                      &lt;span class="nt"&gt;&amp;lt;string&amp;gt;&lt;/span&gt;four2_name&lt;span class="nt"&gt;&amp;lt;/string&amp;gt;&lt;/span&gt;
                    &lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
                  &lt;span class="nt"&gt;&amp;lt;/object&amp;gt;&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
              &lt;span class="nt"&gt;&amp;lt;/object&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;/array&amp;gt;&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;/object&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/object&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/var&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/program&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As you can see it works and could probably be parsed with xpath. It's really ugly and if we were to write an xpath for it, it would be unnecessary complicated and long, but it would work! &lt;br /&gt;
If you are already using it to parse javascript somewhere you might just go with it to reduce dependencies if you wish so.&lt;/p&gt;
&lt;h1 id="right"&gt;Right: Converting json to xml and Parsing It With xpath&lt;/h1&gt;
&lt;p&gt;I found two tools and either one of them combined with either &lt;a href="http://lxml.de/"&gt;&lt;code&gt;lxml&lt;/code&gt;&lt;/a&gt; or &lt;a href="https://github.com/scrapy/parsel"&gt;&lt;code&gt;parsel&lt;/code&gt;&lt;/a&gt; selectors create this beautiful, perfect json-crawling combo for your crawler! &lt;/p&gt;
&lt;p&gt;For unaware &lt;code&gt;lxml&lt;/code&gt; is a really great tool for parsing xml and html while &lt;code&gt;parsel&lt;/code&gt; is built on top of it to make it even greater, so I highly recommend checking it out!
Fun fact - it's also used by &lt;a href="https://github.com/scrapy/scrapy"&gt;scrapy&lt;/a&gt; and that's where it originated.&lt;/p&gt;
&lt;p&gt;Getting back to the point, the two tools that are pretty much alternative to each other are &lt;a href="https://github.com/quandyfactory/dicttoxml"&gt;&lt;code&gt;dicttoxml&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/delfick/python-dict2xml"&gt;&lt;code&gt;dict2xml&lt;/code&gt;&lt;/a&gt;. They are essentially the same thing but I thought I'd mention both since I'm not sure which one is better and requires the recognition. &lt;br /&gt;
For sake of being brief I'll show off &lt;code&gt;dicttoxml&lt;/code&gt; + &lt;code&gt;parsel&lt;/code&gt; only:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dicttoxml&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;dicttoxml&lt;/span&gt;
&lt;span class="n"&gt;root&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dicttoxml&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;attr_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# the tree we get:&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;one&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;two&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;four&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
          &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;four1_name&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;four&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;four&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
          &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;four2_name&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
        &lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;four&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;two&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;one&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="o"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we can parse this tree using &lt;code&gt;parsel.Selector&lt;/code&gt; and xpath:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;parsel&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Selector&lt;/span&gt;
&lt;span class="n"&gt;sel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Selector&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# and get the names with&lt;/span&gt;
&lt;span class="n"&gt;sel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xpath&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;//name/text()&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;# [&amp;#39;four1_name&amp;#39;, &amp;#39;four2_name&amp;#39;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Pretty mind blowing how we solved this mess with one 300 loc big package from pypi and one short xpath.&lt;/p&gt;
&lt;h1 id="solving"&gt;Solving Our Example&lt;/h1&gt;
&lt;p&gt;Now that we have chosen a tool let's see how well it works on a real life example we got ourselves at the beginning of this blog: &lt;a href="http://www.ah.nl/producten/product/wi166580/maggi-opkikker-rundvlees"&gt;http://www.ah.nl/producten/product/wi166580/maggi-opkikker-rundvlees&lt;/a&gt;   &lt;/p&gt;
&lt;p&gt;I'm going to spoil you the joy of reverse engineering the products api and tell you the api url in this case is: 
&lt;code&gt;'http://www.ah.nl/service/rest/delegate?url=/producten/product/wi166580/x'&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Lets assume we already have the page source in &lt;code&gt;body&lt;/code&gt; variable and dive in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dicttoxml&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;dicttoxml&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;parsel&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Selector&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Selector&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dicttoxml&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;attr_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# now we can find things very easily!&lt;/span&gt;
&lt;span class="c1"&gt;# sku:&lt;/span&gt;
&lt;span class="n"&gt;sel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xpath&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;//product/id/text()&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;# [u&amp;#39;wi166580&amp;#39;]&lt;/span&gt;
&lt;span class="c1"&gt;# price:&lt;/span&gt;
&lt;span class="n"&gt;sel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xpath&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;//product//pricelabel/now/text()&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extract&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;# [u&amp;#39;0.82&amp;#39;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Mission accomplished! We managed to parse multi-layer monster with very few, simple xpaths and a small package from pipy!&lt;br /&gt;
Personally I wish I started doing this earlier because iterating through monsters like this one key at the time is extremely tedious and it breaks every time the website decides to update something. &lt;br /&gt;
Hopefully this write up can save someone few hours and an early balding. :D&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bernardas Ališauskas</dc:creator><pubDate>Mon, 10 Oct 2016 00:00:00 +0200</pubDate><guid isPermaLink="false">tag:granitosaurus.github.io,2016-10-10:crawling-json.html</guid><category>python</category><category>crawling</category><category>json</category><category>scrapy</category><category>parsel</category><category>dicttoxml</category></item><item><title>Going to Pycon Poland!</title><link>https://granitosaurus.github.io/pycon-pl.html</link><description>&lt;p&gt;Last year in Euro-Python 2015 someone from PyconPL had a lightning talk about PyconPL and since that moment I was sold. However I missed it by few days. This year however, I'm going and I'm taking a short vacation to explore Warsaw too!&lt;/p&gt;
&lt;p&gt;There's a bunch of info about the conference on the &lt;a href="https://pl.pycon.org/2016/about_en.html"&gt;official page&lt;/a&gt; and I don't want to be redundant, but the venue looks awesome and in overall the event looks pretty huge. Primarily I just wanted to share how much money do you need to attend something like this, how do you get there and lastly whether it is worth it.   &lt;/p&gt;
&lt;h1&gt;The route&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Trip route from Tartu to Warsaw" src="https://granitosaurus.github.io/images/pycon-pl-travel.png" /&gt;   &lt;/p&gt;
&lt;p&gt;So we're taking a bus from Tartu -&amp;gt; Riga which is really the most tedious part of this trip. As you can see in the image the trip almost takes &lt;strong&gt;4 hours&lt;/strong&gt; and the bus leaves early and often is packed full. We'll spend two hours in Riga's airport waiting for our flight, which I really don't mind since Riga's airport is really nice. And to finish it off we'll take a &lt;strong&gt;1.25&lt;/strong&gt; hour long flight to Warsaw itself. I love flying, more accurately I like take offs and landings so having such short flights, where majority of the time will be the take off and the landing, sounds great!&lt;br /&gt;
In conclusion, we leave at 7:00 and we should be in Warsaw at 14:35 (warsaw's time). So this ends up being &lt;strong&gt;8.5 hours for 900km&lt;/strong&gt; which really doesn't have a good hour/km ratio per se but considering the down-times and the location it really isn't bad at all! &lt;br /&gt;
Lastly the conference organizes a bus that will take us from Warsaw to Ossa village, which is few kilometers away from Warsaw and there the conference itself will be held. &lt;/p&gt;
&lt;h1&gt;The Cost&lt;/h1&gt;
&lt;h3&gt;Conference&lt;/h3&gt;
&lt;p&gt;Since I was a bit late to register and missed the early bird prices, I had to drop 211€ per person (422€ for two people) for 4 days of the conference, including food and accommodation. This might seem like a lot but compared to other conferences is really little. &lt;br /&gt;
I actually chatted with one of the hosts a bit and he mentioned that pretty much the whole fee goes to the hotel that is hosting the conference, after visiting the website I can understand that since it advertises the rooms at 90€ a night the conference fee seems to be very reasonable indeed!&lt;/p&gt;
&lt;h3&gt;Travel&lt;/h3&gt;
&lt;p&gt;The plane tickets from Riga to Warsaw and back ended up being 62€ per person(125€ for two) which is slightly above from the best I could find. At one point I got 42€ deals pop up but the payment didn't go through and the next day it popped to 62€. I know airline websites are really fishy when it comes to pricing, storing profiles and cookies to jack up the price whenever they see fit but I'm certain this was just an unfortunate coincidence.&lt;br /&gt;
The bus to Riga from Tartu ended up being 30€ per person (60€ in total) both ways.&lt;/p&gt;
&lt;p&gt;So &lt;strong&gt;total ended up being 303€ per person&lt;/strong&gt; which is pretty cheap for a conference 906km away!&lt;br /&gt;
This doesn't include any traveling expenses which I'll be sure to calculated and include in aftermath blogpost!&lt;/p&gt;
&lt;p&gt;The last python conference I went to was EuroPython 2015 and it was super fun, mostly because I got to meet a bunch of coworkers from Scrapinghub, I hope PyconPL can live up to the hype and I'll be sure to post about it either way.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bernardas Ališauskas</dc:creator><pubDate>Mon, 10 Oct 2016 00:00:00 +0200</pubDate><guid isPermaLink="false">tag:granitosaurus.github.io,2016-10-10:pycon-pl.html</guid><category>pycon</category><category>python</category><category>poland</category><category>travel</category></item><item><title>First post. Hello Pelican!</title><link>https://granitosaurus.github.io/installing-pelican.html</link><description>&lt;p&gt;I've decided to start a blog after Python package called Pelican caught my eye.&lt;br /&gt;
Pelican is a tool to generate a static blog from reStructuredText or Markdown input files. And most importantly it looks to be really fun, full python with jinja2 templating, which means it's fully extendable, configurable and modifiable as it's under GPL license.&lt;/p&gt;
&lt;h3&gt;Installing Pelican&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Pelicans are cool" src="https://granitosaurus.github.io/images/pelican-bird.jpg" /&gt;&lt;br /&gt;
The setup for &lt;code&gt;Pelican&lt;/code&gt; is pretty straightforward just run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;~&amp;gt; pip install pelican  &lt;span class="c1"&gt;# Installing Pelican package for python&lt;/span&gt;
~&amp;gt; mdir blog &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; blog  &lt;span class="c1"&gt;# Create and jump into your blog directory!&lt;/span&gt;
~/blog/&amp;gt; pelican-quickstart
    ... &lt;span class="c1"&gt;#answer some simple questions here&lt;/span&gt;
~/blog/&amp;gt; vim content/first-page.md
    ... &lt;span class="c1"&gt;#write your blog here in simple markdown&lt;/span&gt;
~/blog/&amp;gt; pelican content  &lt;span class="c1"&gt;# regenerate website&lt;/span&gt;
~/blog/&amp;gt; &lt;span class="nb"&gt;cd&lt;/span&gt; output
~/blog/output&amp;gt; python -m pelican.server  &lt;span class="c1"&gt;# run pelican server to test locally&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now connect to &lt;code&gt;http://localhost:8000&lt;/code&gt; and there you go!&lt;br /&gt;
You can check &lt;a href="http://docs.getpelican.com/en/latest/content.html#articles-and-pages"&gt;here&lt;/a&gt; for how to template your message how to format your blog entry.&lt;/p&gt;
&lt;h3&gt;Vim markdown highlight for .md files&lt;/h3&gt;
&lt;p&gt;While going through the installation I've noticed that markdown doesn't have highlighting in vim which was peculiar. I found &lt;a href="http://superuser.com/questions/701496/no-syntax-highlight-on-md-files"&gt;this post which describes a simple fix&lt;/a&gt;.&lt;br /&gt;
Simply create directories and file: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;~/.vim/ftdetect/markdown.vim
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;with content: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;au BufNewFile,BufRead *.md  setf markdown
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Theming Pelican&lt;/h3&gt;
&lt;p&gt;The default Pelican theme is pretty great however I stumbled on &lt;a href="https://github.com/alexandrevicenzi/Flex"&gt;flex-theme&lt;/a&gt; on &lt;a href="https://github.com/getpelican/pelican-themes"&gt;pelican theme repo&lt;/a&gt; on github. So that's my choice for now, but I'd like to touch up the color scheme a bit. Check out &lt;a href="http://docs.getpelican.com/en/stable/pelican-themes.html"&gt;&lt;code&gt;pelican-themes&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Configuring Pelican&lt;/h3&gt;
&lt;p&gt;A lot of bells and whistles come straight out of the box with the pelican and your theme. For example to setup Disqus commnets all I had to do is add &lt;code&gt;DISQUS_SITENAME = "granitosaurus"&lt;/code&gt; where &lt;code&gt;granitosaurus&lt;/code&gt; is my registered name of my disqus account.&lt;/p&gt;
&lt;h3&gt;Publishing Pelican&lt;/h3&gt;
&lt;p&gt;Since Pelican generates a static webpage you can use anything to publish it. I decided to use &lt;a href="http://docs.getpelican.com/en/stable/tips.html#user-pages"&gt;github user pages&lt;/a&gt; which is a bit more complicated than the docs make it out to be. For user pages I like to keep the whole source code in branch &lt;code&gt;source&lt;/code&gt; and keep the generated output in &lt;code&gt;master&lt;/code&gt; as per github's user pages rule. Then use &lt;code&gt;ghp-import&lt;/code&gt; to automatically update master code with the most recent  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;git&lt;/span&gt; &lt;span class="n"&gt;checkout&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="n"&gt;source&lt;/span&gt;
&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;pelican&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="n"&gt;publishconf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;ghp&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;output&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="n"&gt;master&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above will make source branch, generate blog and push the output to &lt;code&gt;master&lt;/code&gt; so it's viewable at https://username.github.io &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; The most important bit is to set &lt;code&gt;SITEURL&lt;/code&gt; in your &lt;code&gt;publishconf.py&lt;/code&gt; to &lt;code&gt;https://username.github.io&lt;/code&gt; make sure it's &lt;strong&gt;HTTPS&lt;/strong&gt; since default SITEURL generated by pelican is http and github pages requires https. This took me an hour of messing around to finally figure out.&lt;/p&gt;
&lt;h3&gt;Wrap Up&lt;/h3&gt;
&lt;p&gt;So far Pelican took quite a bit of work to get things going. It looks quite simple but there's a bunch of little quirks that are really hard to debug. It's not as easy as starting up a wordpress blog but it's quite fun and it seems to be really flexible. &lt;br /&gt;
Let's see if it pays off! &lt;/p&gt;
&lt;p&gt;Checkout the source for more at https://github.com/Granitas/granitas.github.io/tree/source&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bernardas Ališauskas</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0200</pubDate><guid isPermaLink="false">tag:granitosaurus.github.io,2016-10-09:installing-pelican.html</guid><category>pelican</category><category>python</category><category>blog</category></item><item><title>How to get scrapy help.</title><link>https://granitosaurus.github.io/scrapy-help.html</link><description>&lt;p&gt;Scrapy is a web-scraping framework for python. It's pretty popular and at the moment of writing it has over 16000 stars &lt;a href="https://github.com/scrapy/scrapy"&gt;on github&lt;/a&gt;. In terms of codebase scrapy is pretty simple, however there are few things that are not explicit as they could be in favor of abstraction and development simplicity.
So if you do end up not understanding something or encountering some of the few scrapy's quirks, how do you go about it?&lt;/p&gt;
&lt;p&gt;First thing you should do is read &lt;a href="http://stackoverflow.com/help/how-to-ask"&gt;how to ask a good question on stackoverflow&lt;/a&gt;. &lt;br /&gt;
The second bit you should do is learn how to produce a &lt;code&gt;log&lt;/code&gt;. Scrapy logs majority of the events that happen in your spider, so to debug your spider the best resources are these logs.&lt;br /&gt;
To save a log of your spider run you can use UNIX output redirection syntax:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;scrapy crawl myspider 2&amp;gt;&amp;amp;1 &amp;gt; mylog.log
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Explanation:
    &lt;code&gt;scrapy crawl myspider&lt;/code&gt; - is a scrapy command that will start crawling spider called &lt;code&gt;myspider&lt;/code&gt;&lt;br /&gt;
&lt;code&gt;2&amp;gt;&amp;amp;1&lt;/code&gt; - is UNIX syntax for redirecting error output to standard output. In UNIX there are types of outputs and in your log you want to have both of them in one file.&lt;br /&gt;
&lt;code&gt;&amp;gt; mylog.log&lt;/code&gt; - is another UNIX output redirection, but this time we redirect the output to file called &lt;code&gt;mylog.log&lt;/code&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Bernardas Ališauskas</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0200</pubDate><guid isPermaLink="false">tag:granitosaurus.github.io,2016-10-09:scrapy-help.html</guid><category>scrapy</category><category>python</category></item></channel></rss>