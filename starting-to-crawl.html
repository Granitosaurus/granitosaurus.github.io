<!DOCTYPE html>
<html lang="en">
<head>
        <title>Diving Into Web-Crawling</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.4.0/css/font-awesome.min.css" />
        <link rel="stylesheet" href="https://granitosaurus.github.io/theme/css/main.css" />
        <link href="https://granitosaurus.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Blog of Bernardas Ališauskas Full Atom Feed" />
        <link href="https://granitosaurus.github.io/feeds/code.atom.xml" type="application/atom+xml" rel="alternate" title="Blog of Bernardas Ališauskas Categories Atom Feed" />

        <meta name="author" content="Bernardas Ališauskas" />
        <meta name="description" content="Where to start with the art of hoarding online data?" />
<meta property="og:site_name" content="Blog of Bernardas Ališauskas"/>
<meta property="og:title" content="Diving Into Web-Crawling"/>
<meta property="og:description" content="Where to start with the art of hoarding online data?"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://granitosaurus.github.io/starting-to-crawl.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-12-10 00:00:00+01:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://granitosaurus.github.io/author/bernardas-alisauskas.html">
<meta property="article:section" content="code"/>
<meta property="article:tag" content="scrapy"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="web-crawling"/>
<meta property="og:image" content="https://granitosaurus.github.io/images/core/me.jpg">
</head>
<body>

    <div class="navigation pure-menu pure-menu-horizontal">
        <a href="https://granitosaurus.github.io/" class="pure-menu-heading  pure-menu-link">Blog of Bernardas Ališauskas</a>
        <ul class="pure-menu-list">
            <li class="pure-menu-item"></li>
            <li class="pure-menu-item"><a href="/archives.html" class="pure-menu-link">Archives</a></li>
            <li class="pure-menu-item"><a href="/categories.html" class="pure-menu-link">Categories</a></li>
            <li class="pure-menu-item"><a href="/tags.html" class="pure-menu-link">Tags</a></li>

            <li class="pure-menu-item"><a href="https://granitosaurus.github.io/pages/about.html" class="pure-menu-link">About</a></li>
            <li class="pure-menu-item"><a href="https://granitosaurus.github.io/pages/contact.html" class="pure-menu-link">Contact</a></li>
            <li class="pure-menu-item pure-menu-selected"><a href="https://granitosaurus.github.io/category/code.html" class="pure-menu-link">code</a></li>
            <li class="pure-menu-item"><a href="https://granitosaurus.github.io/category/self.html" class="pure-menu-link">self</a></li>
        </ul>
    </div>


<div class="page-container">
    <div class="entry-content">
        <div class="post-meta pure-g">

<div class="pure-u-3-4 meta-data">
    <a href="https://granitosaurus.github.io/category/code.html" class="category">code</a></br>
    <a href="https://granitosaurus.github.io/tag/scrapy" class="tag">scrapy</a>
    <a href="https://granitosaurus.github.io/tag/python" class="tag">python</a>
    <a href="https://granitosaurus.github.io/tag/web-crawling" class="tag">web-crawling</a>
    </br>
    <a class="author" href="https://granitosaurus.github.io/author/bernardas-alisauskas.html">Bernardas Ališauskas</a>
    &mdash; <abbr title="2018-12-10T00:00:00+01:00">Mon 10 December 2018</abbr>
</div>        </div>
    </div>

    <div class="article-header-container">
        <div class="background-image-container">

            <div class="background-image-small">
                <div class="title-container">
                    <h1>Diving Into Web-Crawling</h1>
                </div>
            </div>
        </div>
    </div>

    <div class="entry-content">
        <p>Web crawling is a brilliant source to bootstrap your application. Almost every application requires data of some sort and why not just pick up some public data available on world wide web!</p>
<p>In this introduction I'll cover the core ideas behind web-crawling and web-crawling with python.</p>
<h1>What's Web Crawling?</h1>
<p>First it's important to wrap your head around the stages of web crawling program.  </p>
<blockquote>
<p><strong>crawler</strong> or <strong>spider</strong>
A program that connects to web pages and downloads their contents. </p>
</blockquote>
<p>To put it simply it's a program that goes online and finds two things:  </p>
<ol>
<li>data the user is looking for.  </li>
<li>more targets to crawl </li>
</ol>
<p>e.g.</p>
<ol>
<li>Go to http://shop.com</li>
<li>Find product pages</li>
<li>Find and download product data such as price, title, description</li>
</ol>
<h1>Stages</h1>
<p><img alt="basic crawl loop" src="https://granitosaurus.github.io/images/crawl-loop.png"></p>
<p>A usual web-crawler program will be made from 4 core stages:</p>
<ol>
<li>Discovery - find product urls to crawl</li>
<li>Consumer - consume product urls and retrieve their htmls</li>
<li>Parser - parse html data into something useful</li>
<li>Processor - process the data with pipeline, filters etc.</li>
<li>Loop!</li>
</ol>
<p><em>note: these stages don't have to have to happen in strict order</em></p>
<p>There can be few more steps in between but the core logic of a web crawler looks like this:</p>
<p><img alt="basic crawler" src="https://granitosaurus.github.io/images/crawling.png"></p>
<p><em>worth noting that websites can return all sorts of content not only html. Some return json, some just text and some code like javascript.</em></p>
<p>In very simplest python pseudo code for a crawler would look like this:</p>
<div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://www.python.org/jobs/&#39;</span>

<span class="n">job_urls</span> <span class="o">=</span> <span class="n">find_job_urls</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">consume</span><span class="p">(</span><span class="n">job_urls</span><span class="p">)</span>
<span class="n">processed_data</span> <span class="o">=</span> <span class="n">process</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">json</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;output.json&#39;</span><span class="p">,</span><span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
</pre></div>


<p>You can find full crawler at the end of blog post.</p>
<h1>Crawling</h1>
<p>There are two ways to crawl http urls: </p>
<ul>
<li>Synchronous - slow and simple </li>
<li>Asynchronous - complex but blazing fast.  </li>
</ul>
<blockquote>
<p><strong>Which one to use?</strong> As beginner or if you don't need speed stick with synchronous approach. The programming is much more simple and easier to debug. However asynchronous knowledge is very valuable and learning it is a great idea</p>
</blockquote>
<h2>Synchronous</h2>
<p>This is very straight-forward approach. Everything goes in your program's order: </p>
<p><img alt="basic crawl loop" src="https://granitosaurus.github.io/images/crawl-loop.png"></p>
<blockquote>
<p>for every url: download page -&gt; parse it -&gt; store it -&gt; repeat</p>
</blockquote>
<p>For synchronous crawling most popular library is <a href="http://docs.python-requests.org/en/master/">requests</a></p>
<p>However while this approach is simple and easy to maintain it ends up being very slow as every time program connects to a webpage to download data program need to wait for it to response - in the mean time program could be doing something else: like download <em>another</em> webpage, parse the data or store it.</p>
<blockquote>
<p><strong>when is synchronous approach good enough?</strong> Often a lot of applications don't need to retrieve a lot of data (e.g. football match score crawler) then sync code is more than enough. <br>
Alternatively a lot of website have request limits that are high enough that match synchronous code slowness. </p>
</blockquote>
<h2>Asynchronous</h2>
<p>Async programming is a bit more complex to wrap your head around but to put it shortly it allows the code to be executed in pararel.  </p>
<p>Your program can schedule 100 request to a website <em>at once</em> and handle response as they come in or do something else!  </p>
<p><img alt="async crawl loop" src="https://granitosaurus.github.io/images/crawl-loop-async.png"></p>
<p>So it terms of speed it vastly outperforms synchronous crawlers as they don't have to wait!</p>
<p><img alt="basic crawler" src="https://granitosaurus.github.io/images/sync_v_async.png"></p>
<p><em>"checking" square here is very simplified representation of async mechanism</em></p>
<p>For asynchronous crawling there are a lot of choices and no clear defacto standards. I recommend <a href="https://github.com/ross/requests-futures">requests-futures</a> for caroutine based approach and <a href="https://twistedmatrix.com/">twisted</a> for callback based approach.</p>
<blockquote>
<p><strong>caroutines or callbacks?</strong> While caroutines are much more favored async principle these days callbacks have a special place in web-crawling community as the logic tends to match scraping patterns better</p>
</blockquote>
<h1>Parsing</h1>
<p>There are all sorts of data types on the web, but most likely you're either will be crawling <code>html</code> or <code>json</code>. </p>
<h2>Html</h2>
<p><code>Html</code> is a subset of xml tree structures. It's a great data type for representing structure, however it's not a great data type for representing data itself.</p>
<div class="highlight"><pre><span></span>    <span class="nt">&lt;html&gt;</span>
        <span class="nt">&lt;body&gt;</span>
            <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">name</span><span class="nt">&gt;</span> rubber chicken <span class="nt">&lt;/div&gt;</span>
            <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">price</span><span class="nt">&gt;</span> 55.99 <span class="nt">&lt;/div&gt;</span>
        <span class="nt">&lt;/body&gt;</span>
    <span class="nt">&lt;/html&gt;</span>
</pre></div>


<p>So usually this data is either converted to database tables or json, csv documents:</p>
<ul>
<li>
<p>csv documents:  </p>
<div class="highlight"><pre><span></span>name,price   
rubber chicken, 55.99
</pre></div>


</li>
<li>
<p>json documents:</p>
<div class="highlight"><pre><span></span>[
    {
        &quot;name&quot;: &quot;rubber chicken&quot;,
        &quot;price&quot;: 55.99
    } 
]
</pre></div>


</li>
<li>
<p>database tables</p>
<div class="highlight"><pre><span></span>------------------------------
|       name       |  price  |
------------------------------
|  rubber chicken  |  55.99  |
</pre></div>


</li>
</ul>
<blockquote>
<p><strong>What data type to chose for output?</strong> Anything works! However some data types are easier to work than others. <code>json</code> is an easy to format to work with as it translates to python <code>dict</code> seamlessly. <code>csv</code> is great to work with as it's an easy format to write and parse. <code>json-lines</code> format is best of both worlds. 
<strong>What about data bases?</strong> Document based databases are often preferred when web-scraping. Like <a href="https://www.mongodb.com/">MongoDB</a> and <a href="http://couchdb.apache.org/">couchDB</a>, they are great for storing json data, while relation databases are a bit less straight-forward but come with their own benefits (like <a href="https://sqlite.org/index.html">sqlite</a> and <a href="https://mariadb.org/">mariadb</a>)</p>
</blockquote>
<h3>Parsing</h3>
<p>Since html is a structural data type we can parse it quite easily. For that there are <a href="https://www.w3schools.com/xml/xpath_intro.asp">xpath</a> and <a href="https://www.w3schools.com/csSref/css_selectors.asp">css</a> selectors and appropriate python libraries that implement this selector logic.</p>
<p>For python all you need is <a href="https://github.com/scrapy/parsel">parsel</a> which allows you to use both types of selectors to parse data. Alternatively you can also use a popular alternative <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">beautifulsoup4</a></p>
<p>Here's a <code>parsel</code> example</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">parsel</span> <span class="kn">import</span> <span class="n">Selector</span>

<span class="n">html</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    &lt;html&gt;</span>
<span class="s2">        &lt;body&gt;</span>
<span class="s2">            &lt;div id=name&gt; rubber chicken &lt;/div&gt;</span>
<span class="s2">            &lt;div id=price&gt; 55.99 &lt;/div&gt;</span>
<span class="s2">        &lt;/body&gt;</span>
<span class="s2">    &lt;/html&gt;</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">selector</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">html</span><span class="p">)</span>

<span class="c1"># css selector</span>
<span class="n">name</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;#name::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>

<span class="c1"># xpath selector</span>
<span class="n">price</span> <span class="o">=</span> <span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&quot;price&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">price</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
<span class="c1"># &#39; 55.99 &#39;,&#39; rubber chicken &#39;</span>
</pre></div>


<p>Surprisingly simple!</p>
<blockquote>
<p><strong>Which one, css or xpath?</strong>
Generally css selectors are much easier and more bautiful but xpath selectors are much powerful. So ideally use css and fallback to xpath when encountering something more complicated.  </p>
<p><strong>Parsing with regex?</strong>
Generally parsing html with regex is a bad idea as regex patterns will quickly become unreliable. Html is structure data type - embrace it!</p>
</blockquote>
<h2>Json</h2>
<p>This type of data is often used by website internally together with javascript. Sometimes you can crawl these pages directly either through public or internal websites API. </p>
<p>It's super convenient as you don't need to do any parsing yourself! </p>
<h4>Parsing</h4>
<p>Parsing json is super easy as it can be read as python dictionary right out of the box:</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>
<span class="n">json_data</span> <span class="o">=</span> <span class="s1">&#39;{&quot;data&quot;: [{&quot;name&quot;: &quot;product&quot;, &quot;price&quot;: 55.99}]}&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">json_data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;price&#39;</span><span class="p">])</span>
<span class="c1"># 55.99</span>
</pre></div>


<p><em>These days more and more web pages become dependant on javascript thus often providing json data</em></p>
<h1>Synchronous Example</h1>
<p>Lets write a simple article crawler for python blog posts!</p>
<p>for this we'll use <a href="https://github.com/scrapy/parsel">parsel</a> and <a href="http://docs.python-requests.org/en/master/">requests</a> packages. You can get them via pip:</p>
<div class="highlight"><pre><span></span>pip install parsel requests
</pre></div>


<p>Our spider logic would follow:</p>
<ol>
<li>Go to page with all links to blog posts</li>
<li>Go to every blog post</li>
<li>Extract data</li>
<li>Store data to file</li>
</ol>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">urllib.parse</span> <span class="kn">import</span> <span class="n">urljoin</span><span class="p">,</span> <span class="n">unquote</span>

<span class="kn">from</span> <span class="nn">parsel</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">session</span><span class="p">()</span>  <span class="c1"># 1</span>


<span class="k">def</span> <span class="nf">discover</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Discover job urls in jobs listing page&quot;&quot;&quot;</span>
    <span class="n">jobs_url</span> <span class="o">=</span> <span class="s1">&#39;https://www.python.org/jobs/&#39;</span>

    <span class="n">jobs_html</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">jobs_url</span><span class="p">)</span><span class="o">.</span><span class="n">text</span>
    <span class="n">jobs_sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">jobs_html</span><span class="p">)</span>

    <span class="n">urls</span> <span class="o">=</span> <span class="n">jobs_sel</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;.listing-company-name a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">urljoin</span><span class="p">(</span><span class="n">jobs_url</span><span class="p">,</span> <span class="n">url</span><span class="p">)</span> <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">html</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Parse job html&quot;&quot;&quot;</span>
    <span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">html</span><span class="p">)</span>

    <span class="n">company</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;.listing-location a::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
    <span class="n">email</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;.reference.external::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h2[contains(text(),&#39;Job Title&#39;)]&quot;</span>
                      <span class="s2">&quot;/following-sibling::text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">description</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h2[contains(text(),&#39;Job Title&#39;)]&quot;</span>
                            <span class="s2">&quot;/following-sibling::p/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="n">url</span><span class="p">,</span>
        <span class="s1">&#39;location&#39;</span><span class="p">:</span> <span class="n">company</span><span class="p">,</span>
        <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">title</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span>
        <span class="s1">&#39;description&#39;</span><span class="p">:</span> <span class="n">description</span><span class="p">,</span>
        <span class="s1">&#39;email&#39;</span><span class="p">:</span> <span class="n">unquote</span><span class="p">(</span><span class="n">email</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
    <span class="p">}</span>


<span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Here usually you&#39;d add extra processing steps</span>
<span class="sd">    like upload to database</span>
<span class="sd">    or adding crawl time</span>
<span class="sd">    or determening whether to drop results based on their values&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">data</span>


<span class="k">def</span> <span class="nf">consume</span><span class="p">(</span><span class="n">urls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Consume job urls by downloading them, parsing data and saving to disk&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;crawling job: {url}&#39;</span><span class="p">)</span>
        <span class="n">html</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">text</span>
        <span class="k">yield</span> <span class="n">process</span><span class="p">(</span><span class="n">parse</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">html</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">crawl</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Complete crawl loop:</span>
<span class="sd">    1. Discover job listing urls</span>
<span class="sd">    2. parse html for data</span>
<span class="sd">    3. process data</span>
<span class="sd">    4. store data</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">urls</span> <span class="o">=</span> <span class="n">discover</span><span class="p">()</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">consume</span><span class="p">(</span><span class="n">urls</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;collected.json&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">crawl</span><span class="p">()</span>
</pre></div>


<p>As you can see the crawler is split into 4 parts:</p>
<ul>
<li>Discovery - discover job urls in listing page</li>
<li>Consumer - crawl job urls</li>
<li>Parser - parse data from job htmls</li>
<li>Processor - process data and save it to file</li>
</ul>
<blockquote>
<p><strong>1</strong> - connection sessions establish connection to server and keeps it open for more requests, this speeds up crawling and puts less stress on the host.</p>
</blockquote>
<h1>Conclusion</h1>
<p>Data is often the core of application functionality and web-crawling is a great tool to easily take advantage of public data available online.  </p>
<p>Crawling is a diverse, multi-step process with a lot of viable approaches but to start off sticking with syncrhonious <code>requests</code> and html parsers like <code>parsel</code> can be more than enough for most projects.</p>
<p>For further reading it's important to take a look at <strong>web caching</strong> and <strong>rate limiting</strong>, <strong>proxies</strong> and <strong>failure and memory managing</strong>. Finally there's a whole other problem of <strong>scaling</strong> both crawling and data storage when it comes to millions of results. <br>
I'll be covering these in later blogs </p>
    </div>

    <footer>
        <div class="tags">
            <a href="https://granitosaurus.github.io/tag/scrapy.html">scrapy</a>
            <a href="https://granitosaurus.github.io/tag/python.html">python</a>
            <a href="https://granitosaurus.github.io/tag/web-crawling.html">web-crawling</a>
        </div>
    </footer>

    <div class="entry-content">
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            var disqus_shortname = 'granitosaurus';
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
    </div>

</div>



    <footer class="index-footer">

        <a href="https://granitosaurus.github.io/" title="Blog of Bernardas Ališauskas">Blog of Bernardas Ališauskas</a>
        <a href="/archives.html">Archives</a></li>
        <a href="/categories.html">Categories</a></li>
        <a href="/tags.html">Tags</a></li>
        <a href="https://granitosaurus.github.io/category/code.html">code</a>
        <a href="https://granitosaurus.github.io/category/self.html">self</a>

    </footer>

</body>
</html>